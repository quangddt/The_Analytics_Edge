---
title: "Climate Change"
author: "Quang Duong"
date: "June 25, 2017"
output:
    prettydoc::html_pretty:
    theme: tactile
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.

In this problem, we will attempt to study the relationship between average global temperature and several other factors.

The file [climate_change.csv](https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/f43727842620758fda4e204cc0d7d558/asset-v1:MITx+15.071x+2T2017+type@asset+block/climate_change.csv) contains climate data from May 1983 to December 2008. The available variables include: 

- *Year*: the observation year.

- *Month*: the observation month.

- *Temp*: the difference in degrees Celsius between the average global temperature in that period and a reference value. This data comes from the Climatic Research Unit at the University of East Anglia.

- *CO2, N2O, CH4, CFC.11, CFC.12*: atmospheric concentrations of carbon dioxide (CO2), nitrous oxide (N2O), methane  (CH4), trichlorofluoromethane (CCl3F; commonly referred to as CFC-11) and dichlorodifluoromethane (CCl2F2; commonly referred to as CFC-12), respectively. This data comes from the ESRL/NOAA Global Monitoring Division.

  - CO2, N2O and CH4 are expressed in ppmv (parts per million by volume  -- i.e., 397 ppmv of CO2 means that CO2 constitutes 397 millionths of the total volume of the atmosphere)
  
  - CFC.11 and CFC.12 are expressed in ppbv (parts per billion by volume). 

- *Aerosols*: the mean stratospheric aerosol optical depth at 550 nm. This variable is linked to volcanoes, as volcanic eruptions result in new particles being added to the atmosphere, which affect how much of the sun's energy is reflected back into space. This data is from the Godard Institute for Space Studies at NASA.

- *TSI*: the total solar irradiance (TSI) in W/m2 (the rate at which the sun's energy is deposited per unit area). Due to sunspots and other solar phenomena, the amount of energy that is given off by the sun varies substantially with time. This data is from the SOLARIS-HEPPA project website.

- *MEI*: multivariate El Nino Southern Oscillation index (MEI), a measure of the strength of the El Nino/La Nina-Southern Oscillation (a weather effect in the Pacific Ocean that affects global temperatures). This data comes from the ESRL/NOAA Physical Sciences Division.

First, loading gpplot2 library to help creating plots.
```{r results='hold'}
library(ggplot2)
```
We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv into R.
```{r message = FALSE, results='hold'}
climate_change <- read.csv('climate_change.csv')
attach(climate_change)
```
Summary the dataset to see how many observations and variables it has.
```{r results='hold'}
str(climate_change)
summary(climate_change)
```
We see that this dataset has 308 observations and 11 variables. There is no missing data.

Then, we split the data into a training set, consisting of all the observations up to and including 2006, and a testing set consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.
```{r results='hold'}
training_data <- subset(climate_change, Year<=2006)
testing_data <- subset(climate_change, Year>2006)
tail(training_data$Year)
head(testing_data$Year)
```
Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.

```{r results='hold'}
TempRegs <- lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = training_data)
summary(TempRegs)
```
```{r}
cor(training_data)
```
We observe that N20 is highly correlated with CO2, CH4, and CFC.12. While CFC.11 is highly correlated with CH4 and CFC.12

Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables.
```{r}
TempRegs2 <- lm(Temp ~ MEI + TSI + Aerosols + N2O, data = training_data)
summary(TempRegs2)
```
We see that N20 is now a significant variable. In fact, it is the most significant one. 

We have many variables in this problem, and as we have seen above, dropping some from the model does not decrease model quality. R provides a function, step, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2. This trade-off is formalized by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model.

The step function has one argument - the name of the initial model. It returns a simplified model. Use the step function in R to derive a new model, with the full model as the initial model
```{r results='hold'}
TempRegs3 <- step(TempRegs)
summary(TempRegs3)
```
Notice that CH4 has been removed. It is interesting to note that the step function does not address the collinearity of the variables, except that adding highly correlated variables will not improve the R2 significantly. The consequence of this is that the step function will not necessarily produce a very interpretable model - just a model that has balanced quality and simplicity for a particular weighting of quality and simplicity (AIC).

We have developed an understanding of how well we can fit a linear regression to the training data, but does the model quality hold when applied to unseen data?

Using the model produced from the step function, calculate temperature predictions for the testing data set, using the predict function. Then calculate SSE, SST, and R^2^.
```{r}
TempPrediction <- predict(TempRegs3, testing_data)
SSE <- sum((TempPrediction-testing_data$Temp)^2)
SST <- sum((mean(training_data$Temp)-testing_data$Temp)^2)
1-(SSE/SST)
```
